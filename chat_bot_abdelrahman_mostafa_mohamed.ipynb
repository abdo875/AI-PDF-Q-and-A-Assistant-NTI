{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SreIXWAF-eCN"
      },
      "outputs": [],
      "source": [
        "!pip install -q fastapi uvicorn pyngrok nest_asyncio jinja2 \\\n",
        "               langchain langchain-community langchain-groq chromadb pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "NGROK_AUTH_TOKEN = \"316pNuAwhjYD8giBKQblyXolHAf_6r1FYADkHEz72xGogbbik\"\n"
      ],
      "metadata": {
        "id": "VF44ApqYASo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken $NGROK_AUTH_TOKEN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqV00HYCIHTH",
        "outputId": "c331a6c7-3d69-4062-ee14-06c012f5c7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code = r'''\n",
        "import os\n",
        "from typing import List, Optional\n",
        "\n",
        "from fastapi import FastAPI, Request, Form, UploadFile, File\n",
        "from fastapi.responses import HTMLResponse, RedirectResponse\n",
        "from fastapi.templating import Jinja2Templates\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ---------- Paths & Globals ----------\n",
        "UPLOAD_DIR = \"/content/uploads\"\n",
        "DB_DIR = \"/content/chroma_db\"\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "os.makedirs(DB_DIR, exist_ok=True)\n",
        "\n",
        "templates = Jinja2Templates(directory=\"templates\")\n",
        "app = FastAPI()\n",
        "\n",
        "# Chat state\n",
        "CHAT_HISTORY: List[tuple] = []\n",
        "SYSTEM_PROMPT_DEFAULT = (\n",
        "    \"You are a helpful assistant that answers ONLY using information from the provided documents. \"\n",
        "    \"If you don‚Äôt find the answer in the documents, say you don‚Äôt know and ask the user to upload \"\n",
        "    \"more relevant PDFs. Keep answers concise and in English.\"\n",
        ")\n",
        "current_system_prompt = SYSTEM_PROMPT_DEFAULT\n",
        "\n",
        "# LLM (Groq)\n",
        "llm = ChatGroq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        "    model=\"llama3-8b-8192\",\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Vector DB / QA chain\n",
        "vectordb: Optional[Chroma] = None\n",
        "qa_chain: Optional[RetrievalQA] = None\n",
        "\n",
        "\n",
        "# ---------- Helper to (re)build the vector DB + chain ----------\n",
        "def rebuild_index_and_chain():\n",
        "    global vectordb, qa_chain, current_system_prompt\n",
        "\n",
        "    # Load PDFs\n",
        "    docs = []\n",
        "    for fname in os.listdir(UPLOAD_DIR):\n",
        "        if fname.lower().endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(os.path.join(UPLOAD_DIR, fname))\n",
        "            docs.extend(loader.load())\n",
        "\n",
        "    if not docs:\n",
        "        vectordb = None\n",
        "        qa_chain = None\n",
        "        return\n",
        "\n",
        "    # Split\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    # Embed + index\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectordb = Chroma.from_documents(\n",
        "        chunks,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=DB_DIR\n",
        "    )\n",
        "    vectordb.persist()\n",
        "\n",
        "    # Custom prompt (system_prompt gets injected directly here)\n",
        "    prompt_text = (\n",
        "        f\"{current_system_prompt}\\n\\n\"\n",
        "        \"Context from documents:\\n{context}\\n\\n\"\n",
        "        \"Question: {question}\\n\\n\"\n",
        "        \"Answer in English:\\n\"\n",
        "    )\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=prompt_text\n",
        "    )\n",
        "\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 4}),\n",
        "        chain_type=\"stuff\",\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=False\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------- Routes ----------\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def upload_page(request: Request):\n",
        "    files = [f for f in os.listdir(UPLOAD_DIR) if f.lower().endswith(\".pdf\")]\n",
        "    return templates.TemplateResponse(\"upload.html\", {\"request\": request, \"files\": files})\n",
        "\n",
        "\n",
        "@app.post(\"/upload\", response_class=RedirectResponse)\n",
        "async def upload_files(files: List[UploadFile] = File(...)):\n",
        "    for file in files:\n",
        "        if not file.filename.lower().endswith(\".pdf\"):\n",
        "            continue\n",
        "        out_path = os.path.join(UPLOAD_DIR, file.filename)\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "    rebuild_index_and_chain()\n",
        "    return RedirectResponse(url=\"/chat\", status_code=303)\n",
        "\n",
        "\n",
        "@app.post(\"/reindex\", response_class=RedirectResponse)\n",
        "async def reindex():\n",
        "    rebuild_index_and_chain()\n",
        "    return RedirectResponse(url=\"/chat\", status_code=303)\n",
        "\n",
        "\n",
        "@app.get(\"/chat\", response_class=HTMLResponse)\n",
        "async def chat_page(request: Request):\n",
        "    return templates.TemplateResponse(\"chat.html\", {\n",
        "        \"request\": request,\n",
        "        \"chat_history\": CHAT_HISTORY,\n",
        "        \"system_prompt\": current_system_prompt\n",
        "    })\n",
        "\n",
        "\n",
        "@app.post(\"/chat\", response_class=HTMLResponse)\n",
        "async def chat(request: Request, message: str = Form(...), prompt: str = Form(None)):\n",
        "    global current_system_prompt, qa_chain\n",
        "\n",
        "    # Update system prompt (and rebuild prompt inside chain)\n",
        "    if prompt and prompt.strip():\n",
        "        current_system_prompt = prompt.strip()\n",
        "        rebuild_index_and_chain()\n",
        "\n",
        "    if qa_chain is None:\n",
        "        bot = \"‚ö†Ô∏è No PDFs indexed yet. Upload PDFs first.\"\n",
        "        CHAT_HISTORY.append((\"You\", message))\n",
        "        CHAT_HISTORY.append((\"Bot\", bot))\n",
        "        return templates.TemplateResponse(\"chat.html\", {\n",
        "            \"request\": request,\n",
        "            \"chat_history\": CHAT_HISTORY,\n",
        "            \"system_prompt\": current_system_prompt\n",
        "        })\n",
        "\n",
        "    # Run QA\n",
        "    result = qa_chain.invoke({\"query\": message})\n",
        "    answer = result.get(\"result\", \"‚ö†Ô∏è I couldn‚Äôt generate an answer.\")\n",
        "\n",
        "    CHAT_HISTORY.append((\"You\", message))\n",
        "    CHAT_HISTORY.append((\"Bot\", answer))\n",
        "\n",
        "    return templates.TemplateResponse(\"chat.html\", {\n",
        "        \"request\": request,\n",
        "        \"chat_history\": CHAT_HISTORY,\n",
        "        \"system_prompt\": current_system_prompt\n",
        "    })\n",
        "'''\n",
        "with open(\"main.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "print(\"‚úÖ Wrote main.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0PhfxpwBVLt",
        "outputId": "083dbe3f-b36c-4d1c-a3e1-7445c0ef3b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Wrote main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"templates\", exist_ok=True)\n",
        "\n",
        "upload_html = r\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <meta charset=\"utf-8\"/>\n",
        "  <title>Upload PDFs</title>\n",
        "  <style>\n",
        "    body { font-family: Arial, sans-serif; max-width: 860px; margin: 30px auto; }\n",
        "    .card { background: #fff; border: 1px solid #eee; border-radius: 12px; padding: 16px; margin-bottom: 16px; }\n",
        "    button { padding: 8px 14px; border-radius: 10px; border: 1px solid #ddd; background: #f0f0f0; cursor: pointer; }\n",
        "    input[type=file] { padding: 8px; }\n",
        "    ul { line-height: 1.8; }\n",
        "    a { text-decoration: none; }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>Upload PDFs for RAG</h1>\n",
        "\n",
        "  <div class=\"card\">\n",
        "    <form action=\"/upload\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "      <input type=\"file\" name=\"files\" accept=\".pdf\" multiple>\n",
        "      <button type=\"submit\">Upload & Index</button>\n",
        "    </form>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"card\">\n",
        "    <h3>Uploaded PDFs</h3>\n",
        "    <ul>\n",
        "      {% for f in files %}\n",
        "        <li>{{ f }}</li>\n",
        "      {% endfor %}\n",
        "      {% if files|length == 0 %}\n",
        "        <li><i>No PDFs uploaded yet.</i></li>\n",
        "      {% endif %}\n",
        "    </ul>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"card\">\n",
        "    <form action=\"/reindex\" method=\"post\">\n",
        "      <button type=\"submit\">Rebuild Index</button>\n",
        "      <a href=\"/chat\"><button type=\"button\">Go to Chat</button></a>\n",
        "    </form>\n",
        "  </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "chat_html = r\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <meta charset=\"utf-8\"/>\n",
        "  <title>Chat with your PDFs</title>\n",
        "  <style>\n",
        "    body { font-family: Arial, sans-serif; max-width: 860px; margin: 30px auto; }\n",
        "    .card { background: #fff; border: 1px solid #eee; border-radius: 12px; padding: 16px; margin-bottom: 16px; }\n",
        "    .chat-box { background: #fafafa; border: 1px solid #eee; border-radius: 12px; padding: 14px; min-height: 260px; }\n",
        "    .you  { color: #1b6; }\n",
        "    .bot  { color: #06c; }\n",
        "    textarea, input[type=text] { width: 100%; padding: 10px; border-radius: 10px; border: 1px solid #ddd; }\n",
        "    button { padding: 8px 14px; border-radius: 10px; border: 1px solid #ddd; background: #f0f0f0; cursor: pointer; }\n",
        "    .row { display: flex; gap: 12px; align-items: center; }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>Chat with your PDFs</h1>\n",
        "  <p><a href=\"/\">‚¨ÖÔ∏è Upload more PDFs</a></p>\n",
        "\n",
        "  <div class=\"card\">\n",
        "    <h3>System Prompt (Persona/Rules)</h3>\n",
        "    <form method=\"post\" action=\"/chat\">\n",
        "      <textarea name=\"prompt\" rows=\"5\">{{ system_prompt }}</textarea>\n",
        "      <p><small>Tip: This changes how the model behaves. Keep it concise.</small></p>\n",
        "      <input type=\"text\" name=\"message\" placeholder=\"Type your question about the uploaded PDFs...\"/>\n",
        "      <div class=\"row\">\n",
        "        <button type=\"submit\">Send</button>\n",
        "        <a href=\"/\"><button type=\"button\">Upload PDFs</button></a>\n",
        "      </div>\n",
        "    </form>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"card chat-box\">\n",
        "    {% for sender, text in chat_history %}\n",
        "      <p class=\"{{ 'you' if sender == 'You' else 'bot' }}\"><b>{{ sender }}:</b> {{ text }}</p>\n",
        "    {% endfor %}\n",
        "    {% if chat_history|length == 0 %}\n",
        "      <p><i>No messages yet. Ask something!</i></p>\n",
        "    {% endif %}\n",
        "  </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"templates/upload.html\", \"w\") as f:\n",
        "    f.write(upload_html)\n",
        "\n",
        "with open(\"templates/chat.html\", \"w\") as f:\n",
        "    f.write(chat_html)\n",
        "\n",
        "print(\"‚úÖ Wrote templates/upload.html and templates/chat.html\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAAXx8SYBbQP",
        "outputId": "878e4ec8-187c-4666-8d68-26eeef93c549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Wrote templates/upload.html and templates/chat.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"üîó Public URL:\", public_url)\n",
        "\n",
        "# Start Uvicorn (serves main:app)\n",
        "!uvicorn main:app --host 0.0.0.0 --port 8000 --reload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6jfiixhGpGM",
        "outputId": "1a0c91a9-0d74-472e-9e8a-382758397848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Public URL: NgrokTunnel: \"https://b597e445a051.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m13591\u001b[0m] using \u001b[36m\u001b[1mWatchFiles\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m13593\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "/content/main.py:69: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "2025-08-17 22:36:30.427448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755470190.457561   13593 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755470190.466940   13593 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755470190.497908   13593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755470190.497962   13593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755470190.497966   13593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755470190.497969   13593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-17 22:36:30.506015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/main.py:75: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectordb.persist()\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mPOST /upload HTTP/1.1\u001b[0m\" \u001b[33m303 See Other\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mGET /chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mPOST /chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mPOST /chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     197.58.238.139:0 - \"\u001b[1mPOST /chat HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m13593\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m13591\u001b[0m]\n",
            "/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-DJPEb4GtxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}